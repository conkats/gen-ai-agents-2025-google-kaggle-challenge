{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":835452,"sourceType":"datasetVersion","datasetId":411512},{"sourceId":13415867,"sourceType":"datasetVersion","datasetId":8514720},{"sourceId":13416738,"sourceType":"datasetVersion","datasetId":8515360},{"sourceId":13602494,"sourceType":"datasetVersion","datasetId":8515296}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Prepare dataset and download models","metadata":{}},{"cell_type":"code","source":"!ls /kaggle/input/ecommerce-behavior-data-from-multi-category-store","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T15:27:32.057372Z","iopub.execute_input":"2025-11-10T15:27:32.057717Z","iopub.status.idle":"2025-11-10T15:27:32.212235Z","shell.execute_reply.started":"2025-11-10T15:27:32.057692Z","shell.execute_reply":"2025-11-10T15:27:32.211088Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mkdir -p imgs && cp /kaggle/input/demo-imgs/* imgs/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T15:26:03.031251Z","iopub.execute_input":"2025-11-10T15:26:03.031762Z","iopub.status.idle":"2025-11-10T15:26:03.212271Z","shell.execute_reply.started":"2025-11-10T15:26:03.031734Z","shell.execute_reply":"2025-11-10T15:26:03.211136Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!huggingface-cli download bartowski/nvidia_NVIDIA-Nemotron-Nano-9B-v2-GGUF nvidia_NVIDIA-Nemotron-Nano-9B-v2-Q8_0.gguf --local-dir . --local-dir-use-symlinks False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T15:26:03.266245Z","iopub.execute_input":"2025-11-10T15:26:03.267054Z","iopub.status.idle":"2025-11-10T15:27:05.582741Z","shell.execute_reply.started":"2025-11-10T15:26:03.267023Z","shell.execute_reply":"2025-11-10T15:27:05.581898Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls .","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T15:27:05.584482Z","iopub.execute_input":"2025-11-10T15:27:05.584808Z","iopub.status.idle":"2025-11-10T15:27:05.715837Z","shell.execute_reply.started":"2025-11-10T15:27:05.584785Z","shell.execute_reply":"2025-11-10T15:27:05.715034Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import Image, display\ndisplay(Image(filename='imgs/me.png'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T15:27:05.71682Z","iopub.execute_input":"2025-11-10T15:27:05.717439Z","iopub.status.idle":"2025-11-10T15:27:09.490472Z","shell.execute_reply.started":"2025-11-10T15:27:05.717411Z","shell.execute_reply":"2025-11-10T15:27:09.489511Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# GPU-Accelerated Data Science Agent Tutorial\n\nLearn how to build an AI-powered data science agent with GPU acceleration.\n\n## What You'll Learn\n\n- Enable GPU acceleration in just 2 lines of code\n- Build an AI agent that writes Python code from natural language\n- Perform interactive data analysis through conversational AI\n- Execute large tabular data operations with GPU acceleration","metadata":{}},{"cell_type":"code","source":"display(Image(filename='imgs/pandas.png'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T15:27:09.492976Z","iopub.execute_input":"2025-11-10T15:27:09.493292Z","iopub.status.idle":"2025-11-10T15:27:13.030476Z","shell.execute_reply.started":"2025-11-10T15:27:09.49327Z","shell.execute_reply":"2025-11-10T15:27:13.029668Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## GPU Acceleration in Just 2 Lines of Code\n\n### The Setup\n\nTraditional pandas operations run on CPU. With NVIDIA's `cudf.pandas`, you get massive speedups without changing your code.\n\n```python\nimport cudf.pandas\ncudf.pandas.install()\n```\n\nAfter these two lines, your pandas code automatically runs on GPU when beneficial.","metadata":{}},{"cell_type":"code","source":"display(Image(filename='imgs/agentx.png'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T15:27:13.031194Z","iopub.execute_input":"2025-11-10T15:27:13.031494Z","iopub.status.idle":"2025-11-10T15:27:14.628638Z","shell.execute_reply.started":"2025-11-10T15:27:13.031467Z","shell.execute_reply":"2025-11-10T15:27:14.627775Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## What is the DataScienceAgent?\n\nThe **DataScienceAgent** combines:\n- **Large Language Model (LLM)** - Understands natural language requests\n- **Function Calling** - Executes Python code dynamically based on questions\n- **GPU Acceleration** - Speeds up pandas operations using NVIDIA GPUs\n\n**Key Features:**\n- Natural language interface - ask questions in plain English\n- GPU-accelerated pandas operations for fast data analysis\n- Persistent execution environment - variables carry over between requests\n- Smart error recovery - automatically fixes and retries failed code\n- Automatic data visualization support","metadata":{}},{"cell_type":"code","source":"display(Image(filename='imgs/nemotron-9b-logo.jpg'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T15:27:14.629451Z","iopub.execute_input":"2025-11-10T15:27:14.629773Z","iopub.status.idle":"2025-11-10T15:27:15.41Z","shell.execute_reply.started":"2025-11-10T15:27:14.629743Z","shell.execute_reply":"2025-11-10T15:27:15.409223Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### NVIDIA Nemotron: The Model Behind the Agent\n\nThe DataScienceAgent uses **NVIDIA Nemotron-9B-v2**, a language model optimized for:\n\n- Function calling and structured output\n- Python code generation and data analysis tasks\n- Efficiency - runs locally on consumer GPUs\n- Accuracy - competitive with larger models on specific tasks","metadata":{}},{"cell_type":"code","source":"!cp /kaggle/input/llamacpp-sm75-complete-build/build/bin/llama-server ./ && chmod +x llama-server","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T15:27:15.410777Z","iopub.execute_input":"2025-11-10T15:27:15.411057Z","iopub.status.idle":"2025-11-10T15:27:16.416427Z","shell.execute_reply.started":"2025-11-10T15:27:15.411032Z","shell.execute_reply":"2025-11-10T15:27:16.415235Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import subprocess\nimport os\n\n# Set environment variable\nenv = os.environ.copy()\nenv['LD_LIBRARY_PATH'] = f\"/kaggle/input/llamacpp-sm75-complete-build/build/bin:{env.get('LD_LIBRARY_PATH', '')}\"\n\n# Run the command\nprocess = subprocess.Popen([\n    './llama-server',\n    '-m', '/kaggle/working/nvidia_NVIDIA-Nemotron-Nano-9B-v2-Q8_0.gguf',\n    '--host', '0.0.0.0',\n    '--port', '8000',\n    '-ngl', '99',\n    '--ctx-size', '8192',\n    '--n-predict', '2048',\n    '--threads', '8',\n    '--batch-size', '128',\n    '--ubatch-size', '128',\n    '--cache-reuse', '256',\n    '--flash-attn', 'on',\n    '--reasoning-format', 'none', \n    '--chat-template-file', '/kaggle/input/democode/chat-template-no-think.jinja',\n    '--jinja'\n], env=env)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T15:27:16.417836Z","iopub.execute_input":"2025-11-10T15:27:16.418165Z","iopub.status.idle":"2025-11-10T15:27:16.9066Z","shell.execute_reply.started":"2025-11-10T15:27:16.418131Z","shell.execute_reply":"2025-11-10T15:27:16.905792Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\ntime.sleep(120) # wait for llm server to come online","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Initialize the Agent\n\nInitialize the DataScienceAgent with the local Nemotron server.","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/input/democode')\nfrom agent import DataScienceAgent\ntry:\n    #agent = DataScienceAgent(verbose=True, force_final_response_after_success=True)\n    agent = DataScienceAgent(verbose=True, stream=True, skip_final_response=True)\n    print(\"\\nUsing local Nemotron server at http://localhost:8000\")\n    print(\"Make sure the server is running with: ./start-nemotron-server.sh\\n\")\n    print(\"Agent ready! Type your questions in the cells below.\\n\")\nexcept Exception as e:\n    print(f\"Error initializing agent: {e}\")\n    print(\"\\nMake sure the local LLM server is running.\")\n    print(\"To use NVIDIA cloud API instead, modify agent initialization in this cell\")","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T15:27:16.907504Z","iopub.execute_input":"2025-11-10T15:27:16.90781Z","iopub.status.idle":"2025-11-10T15:27:31.513302Z","shell.execute_reply.started":"2025-11-10T15:27:16.907786Z","shell.execute_reply":"2025-11-10T15:27:31.512121Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### How the Agent Works\n\nThe agent has the following capabilities:\n\n**Tools Available:**\n- `execute_python_code` - Writes and runs pandas code with GPU acceleration\n\n**Intelligence:**\n- Understands natural language questions about data\n- Automatically generates appropriate pandas code with GPU acceleration\n- Handles errors gracefully and retries with corrections\n- Remembers context across the conversation\n\n**Persistent Memory:**\n- Variables (like dataframes) persist between requests\n- Build on previous work without re-loading data\n- Example: Load data in one request, analyze it in the next\n\n**Performance:**\n- GPU acceleration automatically enabled for pandas operations\n- Falls back to CPU if GPU operations aren't supported\n- Execution time tracking for performance monitoring","metadata":{}},{"cell_type":"markdown","source":"## Helper Functions","metadata":{}},{"cell_type":"code","source":"def reset_conversation():\n    \"\"\"Reset the conversation history.\"\"\"\n    agent.reset_conversation()\n    print(\"‚úÖ Conversation reset. Starting fresh!\")\n\ndef ask(prompt):\n    \"\"\"Ask the agent a question and get a response.\"\"\"\n    print(f\"üí¨ You: {prompt}\\n\")\n    print(\"ü§ñ Agent:\")\n\n    # ANSI color codes\n    BOLD = '\\033[1m'\n    GREEN = '\\033[92m'\n    CYAN = '\\033[96m'\n    RED = '\\033[91m'\n    YELLOW = '\\033[93m'\n    MAGENTA = '\\033[95m'\n    BLUE = '\\033[94m'\n    RESET = '\\033[0m'\n\n    try:\n        # For streaming mode: let output appear in real-time, then highlight [TOOL OUTPUT]\n        if agent.stream:\n            import io\n            import sys\n            from IPython.display import display, HTML\n            \n            # Capture output to post-process for highlighting\n            stdout_buffer = io.StringIO()\n            old_stdout = sys.stdout\n            \n            # Use a custom writer that prints immediately AND captures\n            class TeeWriter:\n                def __init__(self, *writers):\n                    self.writers = writers\n                \n                def write(self, text):\n                    for writer in self.writers:\n                        writer.write(text)\n                    return len(text)\n                \n                def flush(self):\n                    for writer in self.writers:\n                        writer.flush()\n            \n            sys.stdout = TeeWriter(old_stdout, stdout_buffer)\n            response = agent.process_prompt(prompt)\n            sys.stdout = old_stdout\n            \n            # Get captured output and apply highlighting to [TOOL OUTPUT] section\n            output = stdout_buffer.getvalue()\n            #print(output)\n            #output = response\n            \n            # Only re-print the TOOL OUTPUT section with colors\n            if True:\n                output = '[Agent Response]\\n'+response\n                lines = output.split('\\n')\n                in_tool_output = False\n                \n                for i, line in enumerate(lines):\n                    if '[Agent Response]' in line:\n                        # Clear the plain [TOOL OUTPUT] and print colored version\n                        print(f\"\\r{BOLD}{MAGENTA}[Agent Response]{RESET}\")\n                        in_tool_output = True\n                    elif in_tool_output and line.strip().startswith('----------------------------------------------------------------------'):\n                        print(f\"\\r{MAGENTA}{line}{RESET}\")\n                        if i > 0 and not any('[Agent Response]' in lines[j] for j in range(max(0, i-5), i)):\n                            in_tool_output = False\n                    elif in_tool_output and line.strip() and not line.strip().startswith('----------------------------------------------------------------------'):\n                        print(f\"\\r{GREEN}{line}{RESET}\")\n            \n        else:\n            # Non-streaming mode: capture and format output\n            import io\n            import sys\n            \n            stdout_buffer = io.StringIO()\n            stderr_buffer = io.StringIO()\n            \n            old_stdout = sys.stdout\n            old_stderr = sys.stderr\n            \n            sys.stdout = stdout_buffer\n            sys.stderr = stderr_buffer\n            response = agent.process_prompt(prompt)\n            sys.stdout = old_stdout\n            sys.stderr = old_stderr\n            \n            # Get the captured output\n            output = stdout_buffer.getvalue()\n            errors = stderr_buffer.getvalue()\n            \n            # Add emoji indicators and formatting to execution results\n            lines = output.split('\\n')\n            formatted_lines = []\n            in_response_section = False\n            \n            for line in lines:\n                # Highlight AGENT RESPONSE section\n                if '[AGENT RESPONSE]' in line:\n                    formatted_lines.append(f\"{BOLD}{CYAN}[AGENT RESPONSE]{RESET}\")\n                    in_response_section = True\n                elif in_response_section and '----------------------------------------------------------------------' in line:\n                    formatted_lines.append(f\"{CYAN}{line}{RESET}\")\n                elif in_response_section and line.strip() and '----------------------------------------------------------------------' not in line and '[' not in line:\n                    # This is the actual response content\n                    formatted_lines.append(f\"{BOLD}{GREEN}{line}{RESET}\")\n                # Add success/failure indicators\n                elif \"'success': True\" in line or '\"success\": true' in line:\n                    formatted_lines.append(f\"‚úÖ {line}\")\n                    in_response_section = False\n                elif \"'success': False\" in line or '\"success\": false' in line:\n                    formatted_lines.append(f\"‚ùå {line}\")\n                    in_response_section = False\n                else:\n                    formatted_lines.append(line)\n                    if line.strip() == '':\n                        in_response_section = False\n            \n            print('\\n'.join(formatted_lines))\n            if errors:\n                import sys\n                print(errors, file=sys.stderr)\n        \n        # Display context token information (works for all modes)\n        context_info = agent.get_context_tokens()\n        print(f\"\\n{BOLD}{YELLOW}üìä Context Usage:{RESET}\")\n        print(f\"  Messages: {context_info['message_count']}\")\n        print(f\"  Total tokens: ~{context_info['total_tokens']:,}\")\n        print(f\"  Breakdown: System={context_info['breakdown_tokens']['system']}, \"\n              f\"User={context_info['breakdown_tokens']['user']}, \"\n              f\"Assistant={context_info['breakdown_tokens']['assistant']}, \"\n              f\"Tool={context_info['breakdown_tokens']['tool']}\")\n\n    except Exception as e:\n        print(f\"‚ùå Error: {e}\")\n        raise\n\n    #return response","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T15:27:31.515986Z","iopub.execute_input":"2025-11-10T15:27:31.516764Z","iopub.status.idle":"2025-11-10T15:27:31.535683Z","shell.execute_reply.started":"2025-11-10T15:27:31.516744Z","shell.execute_reply":"2025-11-10T15:27:31.534922Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### About the Helper Functions\n\nThe cell above defined two convenience functions:\n\n**`reset_conversation()`**\n- Clears the conversation history\n- Resets the execution environment (removes all variables)\n- Useful when starting a fresh analysis\n\n**`ask(prompt)`**\n- Main interface for chatting with the agent\n- Sends your question to the agent\n- Displays formatted responses with color coding:\n  - Green = Successful execution\n  - Red = Errors\n  - Yellow = Context usage statistics\n- Shows token usage to monitor context window","metadata":{}},{"cell_type":"markdown","source":"## Interactive Chat\n\nRun the cells below to interact with the agent. Modify the prompt text or duplicate cells to ask multiple questions.","metadata":{}},{"cell_type":"code","source":"reset_conversation()","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T15:27:31.536397Z","iopub.execute_input":"2025-11-10T15:27:31.536669Z","iopub.status.idle":"2025-11-10T15:27:32.05631Z","shell.execute_reply.started":"2025-11-10T15:27:31.536645Z","shell.execute_reply":"2025-11-10T15:27:32.055387Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\nask(\"Read /kaggle/input/ecommerce-behavior-data-from-multi-category-store/2019-Oct.csv what are the column names?\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T15:28:49.411864Z","iopub.execute_input":"2025-11-10T15:28:49.412222Z","iopub.status.idle":"2025-11-10T15:29:26.204785Z","shell.execute_reply.started":"2025-11-10T15:28:49.412191Z","shell.execute_reply":"2025-11-10T15:29:26.204037Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nask(\"how many rows and columns are there?\")","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T15:29:35.756919Z","iopub.execute_input":"2025-11-10T15:29:35.757563Z","iopub.status.idle":"2025-11-10T15:29:38.073297Z","shell.execute_reply.started":"2025-11-10T15:29:35.757538Z","shell.execute_reply":"2025-11-10T15:29:38.072621Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nask(\"show me the first 5 rows\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T15:29:48.470056Z","iopub.execute_input":"2025-11-10T15:29:48.470955Z","iopub.status.idle":"2025-11-10T15:29:51.829028Z","shell.execute_reply.started":"2025-11-10T15:29:48.470925Z","shell.execute_reply":"2025-11-10T15:29:51.828126Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nask(\"how many unique brands are there?\")","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T15:29:55.706791Z","iopub.execute_input":"2025-11-10T15:29:55.707132Z","iopub.status.idle":"2025-11-10T15:30:01.466709Z","shell.execute_reply.started":"2025-11-10T15:29:55.707106Z","shell.execute_reply":"2025-11-10T15:30:01.465911Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nask(\"what's the most popular brand?\")","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T15:30:16.4566Z","iopub.execute_input":"2025-11-10T15:30:16.456894Z","iopub.status.idle":"2025-11-10T15:30:19.888035Z","shell.execute_reply.started":"2025-11-10T15:30:16.456874Z","shell.execute_reply":"2025-11-10T15:30:19.887374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\nask(\"what is the mean price of samsung?\")","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T15:30:30.025739Z","iopub.execute_input":"2025-11-10T15:30:30.026363Z","iopub.status.idle":"2025-11-10T15:30:33.988101Z","shell.execute_reply.started":"2025-11-10T15:30:30.026339Z","shell.execute_reply":"2025-11-10T15:30:33.987244Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\nask(\"plot the count of the top 10 brand in one bar chart\")","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T15:30:38.456505Z","iopub.execute_input":"2025-11-10T15:30:38.457441Z","iopub.status.idle":"2025-11-10T15:30:45.320551Z","shell.execute_reply.started":"2025-11-10T15:30:38.457408Z","shell.execute_reply":"2025-11-10T15:30:45.319848Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\nask(\"plot the mean price of the top 10 brand in one bar chart\")","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T15:30:50.069952Z","iopub.execute_input":"2025-11-10T15:30:50.070692Z","iopub.status.idle":"2025-11-10T15:30:58.083828Z","shell.execute_reply.started":"2025-11-10T15:30:50.070659Z","shell.execute_reply":"2025-11-10T15:30:58.083059Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## What You Just Learned\n\nYou've built a GPU-accelerated data science agent!\n\n### Key Points\n\n**GPU Acceleration** - Just 2 lines:\n```python\nimport cudf.pandas\ncudf.pandas.install()\n```\n\n**Natural Language Analysis**\n- Ask questions in plain English\n- Agent writes and executes pandas code automatically\n- Variables persist across conversations\n\n**Performance** - GPU speeds up operations on large datasets with no code changes\n\n### Try It Yourself\n\n1. Duplicate any chat cell above\n2. Ask your own questions\n3. Load your own CSV files\n4. Check `agent.py` and `tools.py` to see the implementation\n\n**Remember:** GPU acceleration + LLM function calling = powerful interactive data analysis","metadata":{}},{"cell_type":"code","source":"import psutil\nimport os\nimport signal\n\ndef kill_child_processes(parent_pid=None, sig=signal.SIGTERM):\n    if parent_pid is None:\n        parent_pid = os.getpid()\n    try:\n        parent = psutil.Process(parent_pid)\n    except psutil.NoSuchProcess:\n        return\n    for child in parent.children(recursive=True):\n        try:\n            child.send_signal(sig)\n        except Exception:\n            pass\n\n# Call this at the end of your notebook\nkill_child_processes()\nimport atexit\n\natexit.register(kill_child_processes)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}